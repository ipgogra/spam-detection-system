# -*- coding: utf-8 -*-
"""SMS Spam Detection and Classification Modelling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aq9t_uXzclYNMAexOJzpV3pN5anbO-eZ

## **SMS Spam Detection and Classification System**
Isabella Patterson

# Introduction

The data that is being used here consists of both spam and ham messages. 'Ham' is a term describing messages that generally are good and therefore not spam.  

From this data, a model will be built where messages are able to be identified as either spam or ham. In addition, I will implement the following three classification models:

*   Logistic Regression
*   K-Neighbor
*   Support Vector

# Importing Libraries
First, the necessary libraries will need to be imported:
"""

## Import and load libraries for data handling & manipulation
import numpy as np
import pandas as pd

## Import and load libraries for data visualization
import matplotlib.pyplot as plt
import seaborn as sns

## "" for NLP, NLTK (Natural Language TookKit)
import nltk,re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from wordcloud import WordCloud

## "" Modelling libraries
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split

## "" Classification
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report,plot_confusion_matrix

nltk.download("stopwords")
nltk.download("wordnet")
nltk.download('punkt')

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

"""# Importing Data Set
The following file contains one message per line. There are two columns for each line: *v1* contains the label of either *spam* or *ham* and *v2* contains the raw text/message itself (e.g., "We've been trying to reach you concerning your vehicle's extended warranty...").


*   Dataset: https://www.kaggle.com/uciml/sms-spam-collection-dataset
*   (Original dataset: https://archive.ics.uci.edu/ml/machine-learning-databases/00228/)


Here, the dataset is loaded via downloading and uploading the CSV file to this specific Google Colab session's storage (the original dataset was previously broken at the time, while the Kaggle version has the CSV file available).
"""

## df_sms = pd.read_csv("https://storage.googleapis.com/kagglesdsdata/datasets/483/982/spam.csv?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20220210%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20220210T233056Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=9623be3c88419c162a590da7a53b628be2f8433e709672159f4b6e62a536dff2677b841b57e043e35095cba00aeaf4c5d8ba3e7a2e531055a9e4a71858c1cdadd3aace271c295e6c4347139bcfe6672f195cc635b7e528351f955ebf1fc16f9a0b73bc097d9cee2ef914aa10b3e2e44654d9f5fca34140a76ed75207da4c7563a7d747c816eb711c517ed55f50046895fd0da13087f183459885c54fd73d80c5649a0866c175ad53a0b5bfa46aaefc73a29a9fad549d5f637507d485d88eb724c2403be5ce3d415a8394bcac58bea49827f95cae7b7e76b3219df761825700badaf8fd79360049c4b51bfaeb01c828c7d32a229f217492d346da650904a6d503", encoding = 'latin-1')
df_sms = pd.read_csv("spam.csv", encoding = 'latin-1')
df_sms.head()

"""# Data Cleaning
So, by default, the data is a little disorganized and can be cleaned up a bit. This will be done by removing the unnamed columns, renaming our *v1* and *v2* columns, dropping any duplicate, and/or null values (if any).
"""

df_sms = df_sms.filter(['v1', 'v2'], axis = 1)
df_sms.columns = ['type', 'message']
df_sms.head()

df_sms.drop_duplicates(inplace = True, ignore_index = True)

print('Null values:\n')
df_sms.isnull().sum()

"""Nice ðŸ˜Ž

# Data Analysis
To further explore and analyze our data, a bar graph (or histogram plot) will be implemented.

Specifically, the bar plot shows us the total number of spam and ham messages within the data set. From looking at the results, it looks like there's a lot more ham than spam.

To gain and visualise more insight of our data, a word cloud visual can be used to illustrate the most frequently used words.
"""

sns.countplot(data=df_sms,x ='type')

## word cloud implementation
spam_data = ' '.join(list(df_sms[df_sms['type'] == 'spam']['message']))
word_cloud = WordCloud(width = 600, height = 512).generate(spam_data)
plt.figure(figsize = (13, 9))
plt.imshow(word_cloud)
plt.show()

"""Wow ðŸ¤© A similar word cloud visual can also be applied to ham messages."""

## word cloud of ham words from messages
ham_data = ' '.join(list(df_sms[df_sms['type'] == 'ham']['message']))
word_cloud = WordCloud(width = 600, height = 512).generate(ham_data)
plt.figure(figsize = (13, 9))
plt.imshow(word_cloud)
plt.show()

"""Here, it is interesting to notice the similarities and differences of the words used in spam and ham messages, in addition to their frequency.  Words like "stop", "now", or "please" under the spam message type create a sense of urgency or pressure by being paired with other words like "call", "reply", or "text".  While having similar words and frequencies, the ham message types are seemingly paired with more positive, even casual jargon (like "Lol", "Yup", or "bit", as in "in a bit"). I think this is kind of interesting.

# Preparing Modelling Data

From the previous implementations, this data set has only been examined on a surface level as a whole.  For this next section, lemmatization and tokenization will be conducted.  

Lemmatization reduces a word to its initial 'dictionary' form (lemma), resulting in the word's flexive endings being discarded and the dictionary form of the word is preserved.

Tokenization is a process of breaking down complex data into smaller units, or 'tokens'. This can be done by splitting paragraphs into sentences and sentences into words.

In addition to these two processes, stopwords will need to be removed.  Stopwords are words that frequently occur, and hold meaning within sentences, but are ultimately irrelevant in NLP modelling.

The following are to be replaced:

1.   email addresses >> 'emailaddr'
2.   URLs >> 'httpaddr'
3.   money symbols >> 'moneysymb'
4.   phone numbers >> 'phonenumbr'
5.   numbers >> 'numbr


All punctuations are also to be discarded and all words will be changed to lower case.

Then, the data will need to be converted over into a numerical format in which each word can be represented by a matrix (or vectorization).

After this, the data will need to be split into x train, x test, y train, and y test using the train_test_split() method.
"""

full_data_sms = []
lemmatizer = WordNetLemmatizer()

for i in range(df_sms.shape[0]):
    mess_1 = df_sms.iloc[i, 1]
    mess_1 = re.sub('\b[\w\-.]+?@\w+?\.\w{2,4}\b', 'emailaddr', mess_1)
    mess_1 = re.sub('(http[s]?\S+)|(\w+\.[A-Za-z]{2,4}\S*)', 'httpaddr', mess_1) 
    mess_1 = re.sub('Â£|\$', 'moneysymb', mess_1) 
    mess_1 = re.sub('\b(\+\d{1,2}\s)?\d?[\-(.]?\d{3}\)?[\s.-]?\d{3}[\s.-]?\d{4}\b', 'phonenumbr', mess_1) 
    mess_1 = re.sub('\d+(\.\d+)?', 'numbr', mess_1) 
    mess_1 = re.sub('[^\w\d\s]', ' ', mess_1) 
    mess_1 = re.sub('[^A-Za-z]', ' ', mess_1).lower() 
    token_messages = word_tokenize(mess_1)
    mess = []
    
    for word in token_messages:
        if word not in set(stopwords.words('english')):
            mess.append(lemmatizer.lemmatize(word))
    txt_mess = " ".join(mess)
    full_data_sms.append(txt_mess)

add_df = CountVectorizer(max_features = 1500)
x = add_df.fit_transform(full_data_sms).toarray()
y = df_sms.iloc[:, 0]

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 42)
print('Number of rows in training set: ' + str(x_train.shape))
print('Number of rows in test set: ' + str(x_test.shape))

"""# Classification Models

Finally, the following three classification models:

## **Logistic Regression**
"""

LR = LogisticRegression(random_state = 42, solver = 'liblinear').fit(x_train, y_train)
y_pred_LR = LR.predict(x_test)
print('The first two predicted labels:', y_pred_LR[0], y_pred_LR[1], '\n')
LR_report = classification_report(y_test, y_pred_LR)
print('\t\t\tClassification report:\n\n', LR_report, '\n')
plot_confusion_matrix(LR, x_test, y_test) 
plt.show()

"""## **Support Vector**"""

SVC = SVC(probability = True, random_state = 42).fit(x_train, y_train)
y_pred_SVC = SVC.predict(x_test)
print('The first two predicted labels:', y_pred_SVC[0], y_pred_SVC[1], '\n')
SVC_report = classification_report(y_test, y_pred_SVC)
print('\t\t\tClassification report:\n\n', SVC_report, '\n')
plot_confusion_matrix(SVC, x_test, y_test) 
plt.show()

"""## **K-Neighbour**"""

KN = KNeighborsClassifier(n_neighbors = 3).fit(x_train, y_train)
y_pred_KN = KN.predict(x_test)
print('The first two predicted labels:', y_pred_KN[0], y_pred_KN[1], '\n')
KN_report = classification_report(y_test, y_pred_KN)
print('\t\t\tClassification report:\n\n', KN_report, '\n')
plot_confusion_matrix(KN, x_test, y_test) 
plt.show()

"""## **Conclusion**

Thus, a Spam Detection and Classification Model has been built!

From using Logistic regression, SV, and K-Neighbor Classification models to build and test the end-product model to detect and classify spam from SMS messages, it appears that the SV model outperforms the other classifiers.  While this is the case both in my evaluation and the study this was inspired by, the results may be foundational to further implementations.  Whether this could be extended into sentiment analysis, or advanced AI spam filtering, future work will undoubtedly be explored along with additional features.
"""